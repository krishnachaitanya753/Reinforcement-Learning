# Double DQN for CartPole-v1

This repository contains a PyTorch implementation of Double DQN for the CartPole-v1 environment using [Gymnasium](https://gymnasium.farama.org/). The code supports logging with [Weights & Biases](https://wandb.ai/), video recording, and evaluation.
Only diff between Dqn & Double Dqn is Action is choosen By main network Which reduces overestimation bias and converges to optimal policy faster.
DQN took nearly 450000 steps and Double took around 165000 steps to reach 500 score.
---

## Parameters

The main hyperparameters and settings are defined in the `Args` dataclass:

| Parameter                | Default Value   | Description                                      |
|--------------------------|----------------|--------------------------------------------------|
| `exp_name`               | DDQN_CartPole_3| Experiment name                                  |
| `seed`                   | 42             | Random seed                                      |
| `torch_deterministic`    | True           | Deterministic torch operations                   |
| `cuda`                   | True           | Use CUDA if available                            |
| `track`                  | True           | Enable logging                                   |
| `wandb_project_name`     | CartPole       | WandB project name                               |
| `use_wandb`              | True           | Use WandB logging                                |
| `capture_video`          | False          | Record videos                                    |
| `save_model`             | True           | Save model checkpoints                           |
| `env_id`                 | CartPole-v1    | Gym environment                                  |
| `total_timesteps`        | 300000          | Total training steps                             |
| `learning_rate`          | 2.5e-4         | Adam optimizer learning rate                     |
| `buffer_size`            | 10000          | Replay buffer size                               |
| `gamma`                  | 0.99           | Discount factor                                  |
| `tau`                    | 1              | Target network update rate                       |
| `target_network_frequency`| 500           | Target network update frequency                  |
| `batch_size`             | 128            | Batch size for training                          |
| `start_e`                | 1              | Initial epsilon for exploration                  |
| `end_e`                  | 0.05           | Final epsilon for exploration                    |
| `exploration_fraction`   | 0.5            | Fraction of steps over which to anneal epsilon   |
| `learning_starts`        | 10000           | Steps before training starts                     |
| `train_frequency`        | 10             | Frequency of training updates                    |

---

## Logic & Requirements

- **Environment:** Uses Gymnasium's CartPole-v1.
- **Replay Buffer:** Experience replay via Stable Baselines3's buffer.
- **Double DQN:** 
  - Action selection with the main Q-network.
  - Target value computed using the target Q-network.
- **Target Network:** Soft update controlled by `tau` and `target_network_frequency`.
- **Logging:** Supports TensorBoard and Weights & Biases.
- **Video Recording:** Optionally records and saves gameplay videos.
- **Evaluation:** Periodically evaluates the agent and logs average returns.

### Requirements

- Python 3.10
- torch
- gymnasium
- numpy
- wandb
- opencv-python
- imageio
- tyro
- stable-baselines3

Install dependencies with:
```bash
pip install torch gymnasium numpy wandb opencv-python imageio tyro stable-baselines3
```

---

## Model Size

The Q-network architecture:

- Input: `env.observation_space.shape` (4 for CartPole)
- Hidden Layer 1: 120 units, ReLU
- Hidden Layer 2: 84 units, ReLU
- Output: `env.action_space.n` (2 for CartPole)

**Total parameters:**  
- Input to 120: (4+1)\*120 = 600  
- 120 to 84: (120+1)\*84 = 10164  
- 84 to 2: (84+1)\*2 = 170  
- **Total:** 600 + 10164 + 170 = ** 10934 parameters**

---


### Video

Below is a sample training video generated by the agent (stored in the repo):

![Training Video](final.gif)

### Report


![Analysis](DQN/image.png)

---

## Usage

To train and evaluate the agent:

```bash
python ddqn.py
```


---

## References

- The code is based on [CleanRL's DQN implementation](https://docs.cleanrl.dev/rl-algorithms/dqn/#dqnpy) and [Yuvaraj Singh's DQN implementation](https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DQN).
- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461)
